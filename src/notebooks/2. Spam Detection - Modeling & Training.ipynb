{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmXs_l7TZRS1"
      },
      "source": [
        "# 0. Prerequisites\n",
        "\n",
        "We will be using boto3 in this script later to upload our model to object store. boto3 has some issues with urllib3 and gives an error if we install it after importing other libraries hence we are gonna be intstalling it first in this script\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLQ5NX7zZQUh",
        "outputId": "b2d7e7ab-71e4-4337-b9a8-4d2631d7bbaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.20.12-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting botocore<1.24.0,>=1.23.12\n",
            "  Downloading botocore-1.23.12-py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 33.6 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 55.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.12->boto3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.12->boto3) (1.15.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.7 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.20.12 botocore-1.23.12 jmespath-0.10.0 s3transfer-0.5.0 urllib3-1.26.7\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3\n",
        "# !pip install pandas tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZPDBgY4cApj"
      },
      "source": [
        "# 1. Prepare Training Data\n",
        "\n",
        "Creating a dataset rarely happens next to where you run the training. In the first step we will utilize the Data Preparation notebook that we created earlier to extract the data required to perform training.\n",
        "\n",
        "We will load the pickled data from the Data Preprocessing notebook. While the code uses pickle to load in data, this data is actually exported via pickle when we execute the `%run` in the last block. Since pickle can be unsafe to use from third-party downloaded data, we actually generate (again using `%run`) this pickle data and therefore is safe to use -- it's never downloaded.\n",
        "\n",
        "Please note. We need to re run the data preparation script because we are running the modeling and training script in google colab to utilize the gpus. If we were running the scripts locally we would script the first three block of codes and start from unpickling the training data saved by the data preparation script.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IJqrnVLmTxfG"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RVD_ZKwBOYSq"
      },
      "outputs": [],
      "source": [
        "# path where the data preparation notebook is stored on github\n",
        "DATA_PREPARATION_NOTEBOOK_LINK = \"https://raw.githubusercontent.com/abdullahaleem/spam-detection-microservice/master/app/notebooks/1.%20Spam%20Detection%20-%20Data%20Preparation.ipynb?token=AKTTIMEIAHWZJIIC43VMH4DBU7AT2\"\n",
        "\n",
        "# path where the notebook we created for data preprocessing would be downloaded\n",
        "NOTEBOOKS_DIR = pathlib.Path(\"/notebooks/\")\n",
        "NOTEBOOKS_DIR.mkdir(exist_ok=True, parents=True)\n",
        "DATA_PREPARATION_NOTEBOOK = NOTEBOOKS_DIR / \"Data Preparation.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1_igE41T2GD",
        "outputId": "590ebcf6-27e6-4ccd-99be-953c7072025c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 20648  100 20648    0     0  51620      0 --:--:-- --:--:-- --:--:-- 51620\n"
          ]
        }
      ],
      "source": [
        "!curl $DATA_PREPARATION_NOTEBOOK_LINK  -o \"$DATA_PREPARATION_NOTEBOOK\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI3KbxGIMNrL",
        "outputId": "4c37571c-66db-4a1b-fcb8-f90f16e44197"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  198k  100  198k    0     0   152k      0  0:00:01  0:00:01 --:--:--  152k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  159k  100  159k    0     0   173k      0 --:--:-- --:--:-- --:--:--  172k\n",
            "Archive:  /data/zips/sms-spam-dataset.zip\n",
            "  inflating: /data/spam-classifier/sms_spam/SMSSpamCollection  \n",
            "  inflating: /data/spam-classifier/sms_spam/readme  \n",
            "Archive:  /data/zips/youtube-spam-dataset.zip\n",
            "  inflating: /data/spam-classifier/youtube_spam/Youtube01-Psy.csv  \n",
            "   creating: /data/spam-classifier/youtube_spam/__MACOSX/\n",
            "  inflating: /data/spam-classifier/youtube_spam/__MACOSX/._Youtube01-Psy.csv  \n",
            "  inflating: /data/spam-classifier/youtube_spam/Youtube02-KatyPerry.csv  \n",
            "  inflating: /data/spam-classifier/youtube_spam/__MACOSX/._Youtube02-KatyPerry.csv  \n",
            "  inflating: /data/spam-classifier/youtube_spam/Youtube03-LMFAO.csv  \n",
            "  inflating: /data/spam-classifier/youtube_spam/__MACOSX/._Youtube03-LMFAO.csv  \n",
            "  inflating: /data/spam-classifier/youtube_spam/Youtube04-Eminem.csv  \n",
            "  inflating: /data/spam-classifier/youtube_spam/__MACOSX/._Youtube04-Eminem.csv  \n",
            "  inflating: /data/spam-classifier/youtube_spam/Youtube05-Shakira.csv  \n",
            "  inflating: /data/spam-classifier/youtube_spam/__MACOSX/._Youtube05-Shakira.csv  \n"
          ]
        }
      ],
      "source": [
        "%run \"$DATA_PREPARATION_NOTEBOOK\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXoU08tHT4sH"
      },
      "source": [
        "**If you are running the scripts locally and you have already run Data Preparation script on this system you will start from here.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "R9Jg8xB5Tc-f"
      },
      "outputs": [],
      "source": [
        "# path where the training data would be pickled to using our data prepartation notebook\n",
        "EXPORT_DIR = pathlib.Path('/data/exports/')\n",
        "TRAINING_DATA_PATH = EXPORT_DIR / 'spam-training-data.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "h2ABatrpKmW6"
      },
      "outputs": [],
      "source": [
        "with open(TRAINING_DATA_PATH, 'rb') as f:\n",
        "    data = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-SPvQSPlPFuo"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = data['x_train'], data['y_train']\n",
        "x_test, y_test  = data['x_test'], data['y_test']\n",
        "\n",
        "label_legend = data['label_legend']\n",
        "label_legend_inverted = data['label_legend_inverted']\n",
        "\n",
        "max_sequence_length = data['max_sequence_length']\n",
        "max_words = data['max_words']\n",
        "tokenizer = data['tokenizer']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9S87BLRdvmo"
      },
      "source": [
        "# 2. Create and Train our LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Xzi2CD2hOE4V"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, SpatialDropout1D\n",
        "from tensorflow.keras.models import Model, Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "P159o8O1QNHj"
      },
      "outputs": [],
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "batch_size = 32\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ScyC-JIU81a",
        "outputId": "dda7ab34-5661-463c-93b5-8745c29cd0c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 300, 128)          35840     \n",
            "                                                                 \n",
            " spatial_dropout1d (SpatialD  (None, 300, 128)         0         \n",
            " ropout1D)                                                       \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 196)               254800    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 394       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 291,034\n",
            "Trainable params: 291,034\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embed_dim, input_length=max_sequence_length))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.3))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liwS1atYVzj0",
        "outputId": "6ed39627-37ea-4c92-b89d-c8a31f0c645a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "189/189 [==============================] - 368s 2s/step - loss: 0.2718 - accuracy: 0.8921 - val_loss: 0.1353 - val_accuracy: 0.9568\n",
            "Epoch 2/5\n",
            "189/189 [==============================] - 368s 2s/step - loss: 0.1331 - accuracy: 0.9557 - val_loss: 0.1303 - val_accuracy: 0.9588\n",
            "Epoch 3/5\n",
            "189/189 [==============================] - 369s 2s/step - loss: 0.1233 - accuracy: 0.9621 - val_loss: 0.1284 - val_accuracy: 0.9608\n",
            "Epoch 4/5\n",
            "189/189 [==============================] - 370s 2s/step - loss: 0.1171 - accuracy: 0.9623 - val_loss: 0.1306 - val_accuracy: 0.9588\n",
            "Epoch 5/5\n",
            "189/189 [==============================] - 361s 2s/step - loss: 0.1130 - accuracy: 0.9643 - val_loss: 0.1271 - val_accuracy: 0.9602\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb430156dd0>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, verbose=1, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha2sP2r0ebDi"
      },
      "source": [
        "# 3. Inferencing Trained Model\n",
        "\n",
        "Once we have our model trained we can go ahead test our function on any custom string. We will also write some what of a structure here for inference, which we will build upon in the future"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BdXuKQMcQLWp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Gfbt6BQoV6_k"
      },
      "outputs": [],
      "source": [
        "def predict(text_str, max_words=280, max_sequence=280, tokenizer=None):\n",
        "  if tokenizer:\n",
        "    sequences = tokenizer.texts_to_sequences([text_str])\n",
        "    x_input = pad_sequences(sequences, maxlen=max_sequence)\n",
        "    y_output = model.predict(x_input)\n",
        "    top_y_index = np.argmax(y_output)\n",
        "    preds = y_output[top_y_index]\n",
        "    labeled_preds = [{f\"{label_legend_inverted[i]}\": x} for i, x in enumerate(preds)]\n",
        "    return labeled_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_Yy6wdXe7R6",
        "outputId": "f7af1ba6-d378-47e6-cec1-5de984e4138d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'ham': 0.96547115}, {'spam': 0.03452892}]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict(\"Hello world\", max_words=max_words, max_sequence=max_sequence_length, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WexqrUW1m9IK"
      },
      "source": [
        "# 4. Exporting Model, Tokenizer & Metadata Locally\n",
        " \n",
        "We can load `tokenizer_as_json` with `tensorflow.keras.preprocessing.text.tokenizer_from_json`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "2VPFG3ZSV3Pr"
      },
      "outputs": [],
      "source": [
        "MODEL_EXPORT_PATH = EXPORT_DIR / 'spam-detection-model.h5'\n",
        "model.save(str(MODEL_EXPORT_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpQqUbFvm3CU",
        "outputId": "e6a7f14f-0464-4e0f-d2d8-e3147599be1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "199"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "metadata = {\n",
        "    \"label_legend_inverted\": label_legend_inverted,\n",
        "    \"label_legend\": label_legend,\n",
        "    \"max_sequence_length\": max_sequence_length,\n",
        "    \"max_words\": max_words,\n",
        "}\n",
        "\n",
        "METADATA_EXPORT_PATH = EXPORT_DIR / 'spam-detection-metadata.json'\n",
        "METADATA_EXPORT_PATH.write_text(json.dumps(metadata, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU8LwhYHniqu",
        "outputId": "5acc6282-f473-4b8d-8dd8-6ff23e56c001"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1090335"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer_as_json = tokenizer.to_json()\n",
        "\n",
        "TOKENIZER_EXPORT_PATH = EXPORT_DIR / 'spam-detection-tokenizer.json'\n",
        "TOKENIZER_EXPORT_PATH.write_text(tokenizer_as_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UMBCrswmTt-"
      },
      "source": [
        "# 5. Upload Model, Tokenizer, & Metadata to Object Storage\n",
        "\n",
        "Notebooks on colab are emphemeral and will only keep the files store till our session is active. We also need a place from where our inference scripts which will be deployed will be able to access our model. Hence, we will upload our model to an object store. Over here, we are gonna be using AWS S3 to store our model.\n",
        "\n",
        "Object Storage options include:\n",
        "\n",
        "- AWS S3\n",
        "- Linode Object Storage\n",
        "- DigitalOcean Spaces\n",
        "\n",
        "\n",
        "All three of these options can use `boto3`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "R8WIUPUKObC2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import boto3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky92jkgViuMl"
      },
      "source": [
        "#### AWS S3 Object Storage Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "vg7rXk1-yh3T"
      },
      "outputs": [],
      "source": [
        "# AWS S3 Config\n",
        "ACCESS_KEY = \"AKIA5255GGZVAP3XAKCF\"\n",
        "SECRET_KEY = \"QWbMcekqc2c5vlr9AeO0+X57YNF/Ny+rr9cypxuw\"\n",
        "\n",
        "# You should not have to set this\n",
        "ENDPOINT = None\n",
        "\n",
        "# Your s3-bucket region\n",
        "REGION = 'us-east-1'\n",
        "\n",
        "BUCKET_NAME = 'spam-detection-object-store'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM_5XNwf2vat"
      },
      "source": [
        "#### Linode Object Storage Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVw0G1szyc3T"
      },
      "outputs": [],
      "source": [
        "ACCESS_KEY = \"<your_linode_object_storage_access_key>\"\n",
        "SECRET_KEY = \"<your_linode_object_storage_secret_key>\"\n",
        "\n",
        "# Object Storage Endpoint URL\n",
        "ENDPOINT = \"https://cfe3.us-east-1.linodeobjects.com\"\n",
        "\n",
        "# Object Storage Endpoint Region (also in your endpoint url)\n",
        "REGION = 'us-east-1'\n",
        "\n",
        "# Set this to a valid slug (without a \"/\" )\n",
        "BUCKET_NAME = 'datasets'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4b8n5Np2sLU"
      },
      "source": [
        "#### DigitalOcean Spaces Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wQWHyZyyZto"
      },
      "outputs": [],
      "source": [
        "ACCESS_KEY = \"<your_do_spaces_access_key>\"\n",
        "SECRET_KEY = \"<your_do_spaces_secret_key>\"\n",
        "\n",
        "# Space Endpoint URL\n",
        "ENDPOINT = \"https://ai-cfe-1.nyc3.digitaloceanspaces.com\"\n",
        "\n",
        "# Space Region (also in your endpoint url)\n",
        "REGION = 'nyc3'\n",
        "\n",
        "# Set this to a valid slug (without a \"/\" )\n",
        "BUCKET_NAME = 'datasets'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcNiEDz4yczw"
      },
      "source": [
        "## Perform Upload with Boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "anHupbs23GwB"
      },
      "outputs": [],
      "source": [
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = ACCESS_KEY\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = SECRET_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "0gECGbRgzcAv"
      },
      "outputs": [],
      "source": [
        "# Upload paths \n",
        "MODEL_KEY_NAME = f\"exports/spam-detection/{MODEL_EXPORT_PATH.name}\"\n",
        "TOKENIZER_KEY_NAME = f\"exports/spam-detection/{TOKENIZER_EXPORT_PATH.name}\"\n",
        "METADATA_KEY_NAME = f\"exports/spam-detection/{METADATA_EXPORT_PATH.name}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "mJvboCTTwz_A"
      },
      "outputs": [],
      "source": [
        "session = boto3.session.Session()\n",
        "client = session.client('s3', region_name=REGION, endpoint_url=ENDPOINT)\n",
        "client.upload_file(str(MODEL_EXPORT_PATH), BUCKET_NAME,  MODEL_KEY_NAME) \n",
        "client.upload_file(str(TOKENIZER_EXPORT_PATH), BUCKET_NAME,  TOKENIZER_KEY_NAME) \n",
        "client.upload_file(str(METADATA_EXPORT_PATH), BUCKET_NAME,  METADATA_KEY_NAME)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CYWesuwOvR95"
      },
      "outputs": [],
      "source": [
        "client.download_file(BUCKET_NAME, MODEL_KEY_NAME, pathlib.Path(MODEL_KEY_NAME).name)\n",
        "client.download_file(BUCKET_NAME, TOKENIZER_KEY_NAME, pathlib.Path(TOKENIZER_KEY_NAME).name)\n",
        "client.download_file(BUCKET_NAME, METADATA_KEY_NAME, pathlib.Path(METADATA_KEY_NAME).name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhcGsagoa_MG"
      },
      "source": [
        "# 6. Model Download Pipeline\n",
        "In [this blog post](https://www.codingforentrepreneurs.com/blog/ai-model-download-pipeline) I'll show you how to turn the `client.download_file()` portion into a pipeline so you can make it reusable in future projects. Further, if you ever need to bundle these models into a Docker image, you will be able to use the pipeline.\n",
        "\n",
        "It is not recommended that we upload and manage the model on github as it can get very big. Object store is very good option for storing these models. We don't have versioning and history which we would like to have. \n",
        "\n",
        "We would be downloading three files pretty much every time we deploy this code. We will be using pypyr for automation pipeline (similar to github actions for ci/cd pipelines). We will create a pipeline and a script to download these pipelines in a reperatable manner.\n",
        "\n",
        "The pipeline in created in the code base.\n",
        "\n",
        "We will create a .env which are very common when setting up enviroment variable when working locally. We will never put these files on git. We can have these in product env as well.\n",
        "\n",
        "When we go into production we would have to run `python -m pypyr pipelines/ml-model-download` as step in production\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "q4b8n5Np2sLU"
      ],
      "name": "2. Spam Detection - Modeling & Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
